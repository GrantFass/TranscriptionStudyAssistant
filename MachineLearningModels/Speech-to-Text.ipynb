{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2031e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ddc36b",
   "metadata": {},
   "source": [
    "This is very similar to our Summarization Transformer setup. In this case, processor is analogous to our tokenizer. They both just handle the data before and after the model generates its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0205d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'openai/whisper-tiny'\n",
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint)\n",
    "processor = WhisperProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b0962",
   "metadata": {},
   "source": [
    "Whisper was trained on 16 kHz sampling rate, so our inputs need to match this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60e04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file isn't in the repository, you can replace this with whatever filepath you want \n",
    "#(works on WAV and MP3 for sure, idk about others yet)\n",
    "audio, sr = torchaudio.load(\"audio.wav\") \n",
    "audio = torchaudio.transforms.Resample(sr, 16000)(audio)\n",
    "sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb78d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = processor(audio.squeeze(0), sampling_rate=sr, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90debc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kajan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transcript = model.generate(inputs=input_ids.input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b47b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The birch can use lid on this smooth planks. Glue the sheet to the dark blue background. It is easy to tell the depth of a well. These days, the chicken leg is a rare dish. Rice is often served in round bowls. The juice of lemon makes fine punch. The box was thrown beside the pork truck. The hogs were fed chopped corn and garbage. Four hours of steady work faced us.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(transcript, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f8869",
   "metadata": {},
   "source": [
    "Note - this should start with: \"The birch canoe slid on the smooth planks\"  \n",
    "(First file from: https://www.voiptroubleshooter.com/open_speech/american.html)  \n",
    "This error stems from using the Tiny model and is fixed when using Small. We will see below that Tiny model usually performs very well.  \n",
    "\n",
    "**Whisper cuts out after 30 seconds, so we need to do more chunking (yay).  \n",
    "At the end, all of the chunks will just be joined together to form the final transcript**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbedca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.5 s\n",
      "Wall time: 5.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set up variables for processing audio segments\n",
    "segment_length_seconds = 30\n",
    "segment_length_samples = int(segment_length_seconds * processor.feature_extractor.sampling_rate)\n",
    "\n",
    "# Load the audio file\n",
    "# This is from one of Dr. Magana's Databases lectures from a few years back. I also didn't push this to the repo since it is\n",
    "# a 50 minute long audio file, so again, you can replace this if you want to test it out\n",
    "audio_file_path = \"C:\\\\Users\\\\kajan\\\\Desktop\\\\db1.mp3\"\n",
    "waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "sample_rate = 16000\n",
    "\n",
    "# Split the audio file into segments\n",
    "num_segments = waveform.size(1) // segment_length_samples\n",
    "segments = torch.split(waveform, segment_length_samples, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec7af3",
   "metadata": {},
   "source": [
    "Performance on resampling / splitting isn't bad considering this is a 50 min long audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41cac21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331fe672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 0:  So today we're going to hopefully finish off that ERD lecture. You have some time for questions on lab one for the let's see. But I have now created the submission box for lab one. So that's here. So it's in it's under lab one in week one, right? So that's typically where I'm going to put submission boxes right next to where the assignment was originally uploaded.\n",
      "\n",
      "Segment 1:  So you can go and when you're ready, you can submit your lab one here. I don't think I, let me just edit this for a quick. So when I create a submission box in Canvas, obviously I can give you some directions here, and then the number of points it's worked with the assignment group here is the maps to the syllabus. So it allows, I think, to 40% in this class. So when I set up the grade book,\n",
      "\n",
      "Segment 2:  which I haven't really done yet. As a structure of it, the labs assignment group is going to have the 40% weight and you're great. I'll actually be updated on the fly and you can kind of come check it. Another thing I have here, that is the option of the submission type. This is all pretty basic stuff. But I want to let you know this time I did not restrict the types of fly. I could restrict this to PDFs and zip files, which is\n",
      "\n",
      "Segment 3:  What I was thinking of doing, but in this particular case, I'm actually kind of interested to see how Canvas handles various file types. So I'm not restricting it. You can certainly just, I want you to use a PDF for your report. The other files that you're uploading in this case, there isn't really any code file necessarily. So many students are going to find all I need to do is upload one PDF for the lab report. If you're uploading any supplemental files for whatever reason,\n",
      "\n",
      "Segment 4:  I haven't restricted the file type because I want to see how Canvas handles them. But I do want to maybe actually include this in the description. So one PDF report should be sufficient. So we're not necessarily looking for a bunch of different files in this case. I'm not interested in code files really because we just saved this.\n",
      "\n",
      "Segment 5:  It's underneath. Yep. I'm not looking for code files in this case in future labs. They may be code files, right? So that's listed as a submission box here under. Um, uh, lab. What you'll find, um, I don't necessarily want to click into this because it'll show everybody's, uh, grade, um, although that's only based on this Hello Canvas so far, which I have to enter the points for, but, um, this, um, grade section, which you have access to eventually is going to show you\n",
      "\n",
      "Segment 6:  Well, right now, I would have show you some empty slots, but it'll show you your grade and give you a calculated grade. Blackboard had this capability, but Blackboard's grade book had some issues. I'm not sure how many of your professors heavily used it. I know that for me, I would use Blackboards. I would use submission boxes on Blackboard and enter grades there, but I would actually calculate a grade later in Excel. And Canvas actually has a much more in my opinion robust grade book that we hopefully can just rely on.\n",
      "\n",
      "Segment 7:  directly. And so in about a week or two when you've got some grades in there, that'll look pretty nice right now. It's not going to look very fancy, but it is a good feature of campus. It's an improvement in my opinion over Blackboard. So let's see. Okay. So that's where the submission box for lab one is. And I do reference that in week two down here. I do say that the submission link is in week\n",
      "\n",
      "Segment 8:  on further lab. Another couple little differences here to keep an eye out for between Canvas and Blackboard. So in Blackboard we had the option to have you jump right to a particular module when you open up Blackboard. We as professors could change that so you would see the most recent week or whatever if that's how your professor was using it, that's how I used it. In the case of Canvas, the modules are all listed together here. And so if you are no longer interested in seeing week one, you will be able to collapse it.\n",
      "\n",
      "Segment 9:  And then, you know, we two is there. So that's a little bit of a difference between the two systems just to be aware of. Okay, let's see. We have a mini lab that we're going to start and work on a class on Monday. This mini lab is basically you can look ahead if you like. It's a description of it. Think in art gallery, small business. We're going to try to design a database for them within ERD, right? And we'll stop at the ERD for this one. It's giving us a bunch of text description of what this art gallery needs. And we want to\n",
      "\n",
      "CPU times: total: 49.3 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Iterate over the segments and transcribe each one\n",
    "for i, segment in enumerate(segments[10:20]):\n",
    "    \n",
    "    # Handle multi-channel vs mono audio\n",
    "    if segment.shape[0] > 1:\n",
    "        segment = torch.mean(segment, dim=0, keepdim=False)\n",
    "    else:\n",
    "        segment = segment.squeeze(0)\n",
    "    \n",
    "    input_ids = processor(segment, sampling_rate=sample_rate, return_tensors='pt').input_features\n",
    "    output_ids = model.generate(inputs=input_ids)\n",
    "    transcription = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    print(f\"Segment {i}: {transcription}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720215f",
   "metadata": {},
   "source": [
    "For transcribing 10 / 100 chunks, the performance (speed) again isn't bad considering this is run in my laptop cpu. Will be even faster if we use torch cuda  \n",
    "  \n",
    "Another note, whisper comes in various sizes: tiny, small, medium, large  \n",
    "Smaller models run faster but might not be as accurate. Tiny seems to be doing just fine right now, as it even picks up terms like ERD (entity relationship diagram) from a databases lecture. However, if we notice it performing poorly in the future, we can just use a bigger model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
